import openai
from openai import OpenAI
import os
from datetime import datetime
from config import config
from utils.logger import logger
from typing import List, Tuple

class LLMUtils:
    """
    ğŸ¤– å¤§è¯­è¨€æ¨¡å‹å·¥å…·ç±»
    åŠŸèƒ½ï¼š
    1. ğŸ”— å°è£…OpenAI APIè°ƒç”¨
    2. ğŸ¯ æä¾›æ„å›¾è¯†åˆ«ã€æ–‡æœ¬ç”Ÿæˆã€é—®é¢˜é¢„æµ‹ç­‰åŠŸèƒ½
    3. ğŸ›¡ï¸ å¤„ç†APIé”™è¯¯å’Œé™æµ
    
    è®¾è®¡åŸåˆ™ï¼š
    1. ğŸ” æŠ½è±¡åŒ– - éšè—åº•å±‚APIç»†èŠ‚
    2. âš™ï¸ å¯é…ç½® - æ¨¡å‹å‚æ•°å¯é€šè¿‡é…ç½®è°ƒæ•´
    3. ğŸ›¡ï¸ å¥å£®æ€§ - å¤„ç†APIé”™è¯¯å’Œå¼‚å¸¸
    """
    
    def __init__(self):
        logger.info("ğŸ¤– åˆå§‹åŒ–LLMå·¥å…·ç±»")
        
        # ğŸ”— åˆ›å»ºOpenAIå®¢æˆ·ç«¯å®ä¾‹
        self.client = OpenAI(
            api_key=config.MODEL_API_KEY,
            base_url=config.MODEL_BASE_URL,
        )
        logger.info(f"âœ… LLMé…ç½®å®Œæˆ: {config.MODEL_NAME} @ {config.MODEL_BASE_URL}")
    
    def generate_text(self, prompt: str, max_tokens: int = 1024, temperature: float = 0.7) -> str:
        """
        âœï¸ ä½¿ç”¨LLMç”Ÿæˆæ–‡æœ¬
        å‚æ•°:
            prompt: è¾“å…¥æç¤º
            max_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            temperature: ç”Ÿæˆæ¸©åº¦ (0-1, è¶Šé«˜è¶Šéšæœº)
        è¿”å›: ç”Ÿæˆçš„æ–‡æœ¬
        """
        try:
            logger.info(f"ğŸ¤– å¼€å§‹æ–‡æœ¬ç”Ÿæˆ (max_tokens: {max_tokens}, temperature: {temperature})")
            start_time = datetime.now()
            
            # ğŸ”— ä½¿ç”¨æ–°çš„Chat Completions API
            response = self.client.chat.completions.create(
                model=config.MODEL_NAME,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=max_tokens,
                temperature=temperature
            )
            
            # âœ‚ï¸ æå–ç”Ÿæˆçš„æ–‡æœ¬
            generated_text = response.choices[0].message.content.strip()
            
            # ğŸ“ è®°å½•æ€§èƒ½ä¿¡æ¯
            duration = (datetime.now() - start_time).total_seconds()
            tokens_used = response.usage.total_tokens
            logger.info(f"âœ… LLMç”Ÿæˆå®Œæˆ: {tokens_used} tokens, {duration:.2f}s, å›å¤é•¿åº¦: {len(generated_text)}")
            
            return generated_text
        except openai.APIError as e:  # ğŸš¨ ä½¿ç”¨æ–°çš„é”™è¯¯ç±»å‹
            logger.error(f"âŒ LLM APIé”™è¯¯: {str(e)}")
            return "æŠ±æ­‰ï¼Œæˆ‘æš‚æ—¶æ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜ã€‚ğŸ˜…"
        except Exception as e:
            logger.error(f"âŒ LLMè°ƒç”¨å¼‚å¸¸: {str(e)}")
            return "å¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‘ç”Ÿäº†é”™è¯¯ã€‚ğŸ˜…"
    
    def classify_intent(self, user_input: str) -> str:
        """
        ğŸ¯ ä½¿ç”¨LLMè¿›è¡Œæ„å›¾è¯†åˆ«
        å‚æ•°: ç”¨æˆ·è¾“å…¥æ–‡æœ¬
        è¿”å›: æ„å›¾ç±»åˆ« (A-K)
        """
        logger.info(f"ğŸ¯ å¼€å§‹æ„å›¾åˆ†ç±»: '{user_input[:50]}...'")
        
        # ğŸ“ æ„é€ æç¤º
        prompt = config.INTENT_PROMPT.format(user_input=user_input)
        
        # ğŸ¤– è°ƒç”¨LLM
        intent = self.generate_text(prompt, max_tokens=2, temperature=0.1)
        
        # âœ… éªŒè¯å’Œæ¸…ç†ç»“æœ
        intent = intent.strip().upper()
        valid_intents = ["A", "B", "C", "D", "E", "F", "G", "H", "J", "K"]
        
        if intent in valid_intents:
            logger.info(f"âœ… æ„å›¾è¯†åˆ«æˆåŠŸ: '{user_input[:30]}...' -> {intent}")
            return intent
        else:
            logger.warning(f"âš ï¸ æ— æ³•è¯†åˆ«çš„æ„å›¾: '{intent}'ï¼Œä½¿ç”¨é»˜è®¤å€¼ 'D'")
            return "D"  # â“ æ— æ³•è¯†åˆ«
    
    def predict_next_questions(self, current_input: str, history: List[Tuple[str, str]], max_questions: int = 3) -> List[str]:
        """
        ğŸ”® é¢„æµ‹åç»­å¯èƒ½çš„é—®é¢˜
        å‚æ•°:
            current_input: å½“å‰ç”¨æˆ·è¾“å…¥
            history: å†å²å¯¹è¯åˆ—è¡¨ [(role, content), ...]
            max_questions: æœ€å¤§é¢„æµ‹é—®é¢˜æ•°
        è¿”å›: é¢„æµ‹çš„é—®é¢˜åˆ—è¡¨
        """
        logger.info(f"ğŸ”® å¼€å§‹é¢„æµ‹åç»­é—®é¢˜ (æœ€å¤š{max_questions}ä¸ª)")
        
        # ğŸ“ æ„å»ºå†å²å¯¹è¯ä¸Šä¸‹æ–‡
        history_context = "\n".join([f"{role}: {content}" for role, content in history[-config.MAX_CHAT_HISTORY:]])
        
        # ğŸ“ æ„é€ æç¤º
        prompt = f"""
        ğŸ”® æ ¹æ®å½“å‰å¯¹è¯å†…å®¹å’Œå†å²è®°å½•ï¼Œæ¨æµ‹ç”¨æˆ·æ¥ä¸‹æ¥å¯èƒ½æå‡ºçš„é—®é¢˜ï¼ˆæœ€å¤š{max_questions}ä¸ªï¼‰ï¼š
        
        ğŸ’¬ å½“å‰å¯¹è¯ï¼š
        {current_input}
        
        ğŸ“œ å†å²å¯¹è¯ï¼ˆæœ€è¿‘{config.MAX_CHAT_HISTORY}æ¡ï¼‰ï¼š
        {history_context}
        
        â“ è¯·åˆ—å‡ºå¯èƒ½çš„é—®é¢˜ï¼Œæ¯ä¸ªé—®é¢˜å•ç‹¬ä¸€è¡Œï¼š
        """
        
        # ğŸ¤– è°ƒç”¨LLM
        response = self.generate_text(prompt, max_tokens=200, temperature=0.5)
        
        # ğŸ“‹ è§£æå“åº”ä¸ºé—®é¢˜åˆ—è¡¨
        questions = [q.strip() for q in response.split("\n") if q.strip()]
        
        # âœ‚ï¸ é™åˆ¶é—®é¢˜æ•°é‡
        questions = questions[:max_questions]
        logger.info(f"âœ… é—®é¢˜é¢„æµ‹å®Œæˆ: {questions}")
        return questions

# ğŸŒ å…¨å±€LLMå·¥å…·å¯¹è±¡
llm_utils = LLMUtils()