import os
import uuid
from pathlib import Path
from langchain_community.document_loaders import (
    PyPDFLoader, Docx2txtLoader, TextLoader,
    UnstructuredPowerPointLoader, UnstructuredHTMLLoader, NotebookLoader
)
from langchain_text_splitters import RecursiveCharacterTextSplitter
import chromadb
from chromadb.config import Settings
from config import config
from utils.logger import logger
from utils.llm_utils import llm_utils


class KnowledgeBase:
    """
    ğŸ“š çŸ¥è¯†åº“ç®¡ç†æ¨¡å—
    åŠŸèƒ½ï¼š
      1. ğŸ“ çŸ¥è¯†æ¡ç›®çš„æ·»åŠ ã€åˆ é™¤å’Œæ£€ç´¢
      2. ğŸ” æ–‡æ¡£å†…å®¹çš„å‘é‡åŒ–å­˜å‚¨
      3. ğŸ§  çŸ¥è¯†æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)
      4. ğŸ“‹ å…ƒæ•°æ®ç®¡ç†
    
    è®¾è®¡åŸåˆ™ï¼š
      1. ğŸ”§ æ¨¡å—åŒ–è®¾è®¡ - æ¸…æ™°çš„æ¥å£åˆ’åˆ†
      2. âš¡ é«˜æ•ˆæ£€ç´¢ - ä½¿ç”¨å‘é‡æ•°æ®åº“å®ç°è¯­ä¹‰æœç´¢
      3. ğŸ“ˆ å¯æ‰©å±•æ€§ - æ”¯æŒå¤šç§æ–‡æ¡£æ ¼å¼
      4. ğŸ·ï¸ å…ƒæ•°æ®ç®¡ç† - ä¸°å¯Œçš„çŸ¥è¯†æ¡ç›®æè¿°
    """

    def __init__(self):
        logger.info("ğŸ“š åˆå§‹åŒ–çŸ¥è¯†åº“ç®¡ç†æ¨¡å—")
        
        # ğŸ§© åˆå§‹åŒ–æ–‡æœ¬åˆ†å—å™¨
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=config.CHUNK_SIZE,
            chunk_overlap=config.CHUNK_OVERLAP,
            length_function=len,
            is_separator_regex=False,
        )
        
        # ğŸ—„ï¸ åˆå§‹åŒ–å‘é‡æ•°æ®åº“å®¢æˆ·ç«¯
        self.chroma_client = chromadb.PersistentClient(
            path=str(config.VECTOR_STORE_DIR),
            settings=Settings(allow_reset=True)
        )
        
        # ğŸ“š ä¸»çŸ¥è¯†åº“é›†åˆ
        self.main_collection = self.chroma_client.get_or_create_collection(
            name="main_knowledge_base",
            metadata={"hnsw:space": "cosine"}
        )
        logger.info("âœ… çŸ¥è¯†åº“åˆå§‹åŒ–å®Œæˆ")
    
    def _get_loader(self, file_type: str):
        """æ ¹æ®æ–‡ä»¶ç±»å‹è·å–å¯¹åº”çš„æ–‡æ¡£åŠ è½½å™¨"""
        loader_map = {
            "pdf": PyPDFLoader,
            "docx": Docx2txtLoader,
            "txt": lambda path: TextLoader(path, encoding='utf-8'),
            "pptx": UnstructuredPowerPointLoader,
            "html": UnstructuredHTMLLoader,
            "ipynb": NotebookLoader
        }
        return loader_map.get(file_type.lower())
    
    def add_document(self, file_path: Path, metadata: dict) -> bool:
        """
        ğŸ“¥ æ·»åŠ æ–‡æ¡£åˆ°çŸ¥è¯†åº“
        1. ğŸ“– åŠ è½½æ–‡æ¡£å†…å®¹
        2. âœ‚ï¸ æ–‡æœ¬åˆ†å—å¤„ç†
        3. ğŸ—„ï¸ å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
        4. ğŸ·ï¸ ä¿å­˜å…ƒæ•°æ®
        
        å‚æ•°:
          file_path: æ–‡ä»¶è·¯å¾„
          metadata: å…ƒæ•°æ® (æ ‡é¢˜ã€ä½œè€…ã€æ ‡ç­¾ç­‰)
        è¿”å›:
          æ˜¯å¦æ·»åŠ æˆåŠŸ
        """
        try:
            logger.info(f"ğŸ“¥ å¼€å§‹æ·»åŠ æ–‡æ¡£åˆ°çŸ¥è¯†åº“: {file_path.name}")
            
            # ğŸ“‹ è·å–æ–‡ä»¶ç±»å‹
            file_type = file_path.suffix[1:]  # å»æ‰ç‚¹å·
            
            # ğŸ› ï¸ è·å–å¯¹åº”çš„æ–‡æ¡£åŠ è½½å™¨
            loader_class = self._get_loader(file_type)
            if not loader_class:
                logger.error(f"âŒ ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_type}")
                return False
            
            # ğŸ“– åŠ è½½æ–‡æ¡£
            loader = loader_class(str(file_path))
            documents = loader.load()
            
            if not documents:
                logger.warning(f"âš ï¸ æ–‡æ¡£å†…å®¹ä¸ºç©º: {file_path}")
                return False
            
            # âœ‚ï¸ æ–‡æœ¬åˆ†å—
            chunks = self.text_splitter.split_documents(documents)
            
            # ğŸ“ å‡†å¤‡å‘é‡æ•°æ®åº“å­˜å‚¨
            ids = [str(uuid.uuid4()) for _ in chunks]
            texts = [chunk.page_content for chunk in chunks]
            
            # ğŸ·ï¸ æ·»åŠ å…ƒæ•°æ®
            metadatas = []
            for i, chunk in enumerate(chunks):
                chunk_metadata = {
                    "source": metadata.get("title", file_path.stem),
                    "author": metadata.get("author", "unknown"),
                    "tags": metadata.get("tags", ""),
                    "chunk_index": i,
                    "file_path": str(file_path),
                    **metadata  # åŒ…å«æ‰€æœ‰è‡ªå®šä¹‰å…ƒæ•°æ®
                }
                metadatas.append(chunk_metadata)
            
            # ğŸ—„ï¸ æ·»åŠ åˆ°å‘é‡æ•°æ®åº“
            self.main_collection.add(
                ids=ids,
                documents=texts,
                metadatas=metadatas
            )
            
            logger.info(f"âœ… æ–‡æ¡£æ·»åŠ æˆåŠŸ: {file_path.name} -> {len(chunks)}ä¸ªå—")
            return True
        except Exception as e:
            logger.error(f"âŒ æ·»åŠ æ–‡æ¡£å¤±è´¥: {file_path} - {str(e)}")
            return False
    
    def delete_document(self, file_path: str) -> bool:
        """
        ğŸ—‘ï¸ ä»çŸ¥è¯†åº“ä¸­åˆ é™¤æ–‡æ¡£
        1. ğŸ” æŸ¥æ‰¾æ‰€æœ‰ç›¸å…³å—
        2. ğŸ—‘ï¸ ä»å‘é‡æ•°æ®åº“ä¸­åˆ é™¤
        
        å‚æ•°:
          file_path: æ–‡ä»¶è·¯å¾„
        è¿”å›:
          æ˜¯å¦åˆ é™¤æˆåŠŸ
        """
        try:
            logger.info(f"ğŸ—‘ï¸ å¼€å§‹ä»çŸ¥è¯†åº“åˆ é™¤æ–‡æ¡£: {file_path}")
            
            # ğŸ” æŸ¥æ‰¾æ‰€æœ‰ä¸è¯¥æ–‡ä»¶ç›¸å…³çš„å—
            results = self.main_collection.get(
                where={"file_path": file_path},
                include=["metadatas", "documents"]
            )
            
            if not results['ids']:
                logger.warning(f"âš ï¸ æœªæ‰¾åˆ°ä¸æ–‡ä»¶ç›¸å…³çš„çŸ¥è¯†å—: {file_path}")
                return False
            
            # ğŸ—‘ï¸ åˆ é™¤æ‰€æœ‰ç›¸å…³å—
            self.main_collection.delete(ids=results['ids'])
            
            logger.info(f"âœ… æˆåŠŸåˆ é™¤ {len(results['ids'])} ä¸ªçŸ¥è¯†å—")
            return True
        except Exception as e:
            logger.error(f"âŒ åˆ é™¤æ–‡æ¡£å¤±è´¥: {file_path} - {str(e)}")
            return False
    
    def query(self, question: str, top_k: int = 5) -> tuple:
        """
        ğŸ” çŸ¥è¯†åº“æŸ¥è¯¢
        1. ğŸ” è¯­ä¹‰æ£€ç´¢ç›¸å…³æ–‡æ¡£å—
        2. ğŸ¤– ä½¿ç”¨LLMç”Ÿæˆç­”æ¡ˆ
        å‚æ•°:
          question: æŸ¥è¯¢é—®é¢˜
          top_k: è¿”å›çš„ç›¸å…³å—æ•°é‡
        è¿”å›:
          (ç”Ÿæˆçš„ç­”æ¡ˆ, ç›¸å…³æ–‡æ¡£å—åˆ—è¡¨)
        """
        try:
            logger.info(f"ğŸ” å¼€å§‹çŸ¥è¯†åº“æŸ¥è¯¢: '{question}'")
            
            # ğŸ” è¯­ä¹‰æ£€ç´¢ç›¸å…³æ–‡æ¡£å—
            results = self.main_collection.query(
                query_texts=[question],
                n_results=top_k
            )
            
            # ğŸ“„ æå–ç›¸å…³æ–‡æ¡£å†…å®¹
            context_docs = results['documents'][0]
            metadatas = results['metadatas'][0]
            
            logger.info(f"âœ… æ£€ç´¢åˆ° {len(context_docs)} æ¡ç›¸å…³æ–‡æ¡£")
            
            # ğŸ“ æ„å»ºä¸Šä¸‹æ–‡å­—ç¬¦ä¸²
            context_str = "\n\n".join([
                f"[ğŸ“„ æ¥æº: {meta['source']}, ğŸ‘¤ ä½œè€…: {meta.get('author', 'æœªçŸ¥')}]\n{content}"
                for content, meta in zip(context_docs, metadatas)
            ])
            
            # ğŸ¤– ä½¿ç”¨LLMç”Ÿæˆç­”æ¡ˆ
            prompt = f"""
            ğŸ¤– ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½çŸ¥è¯†åº“åŠ©æ‰‹ï¼Œè¯·æ ¹æ®æä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”é—®é¢˜ã€‚
            å¦‚æœä¸Šä¸‹æ–‡ä¿¡æ¯ä¸è¶³ä»¥å›ç­”é—®é¢˜ï¼Œè¯·å¦‚å®å‘ŠçŸ¥ã€‚
            
            â“ é—®é¢˜ï¼š
            {question}
            
            ğŸ“š ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
            {context_str}
            
            ğŸ’¡ è¯·åŸºäºä»¥ä¸Šä¿¡æ¯æä¾›å‡†ç¡®ã€å®Œæ•´çš„å›ç­”ï¼š
            """
            
            answer = llm_utils.generate_text(prompt)
            return answer, context_docs
        except Exception as e:
            logger.error(f"âŒ çŸ¥è¯†åº“æŸ¥è¯¢å¤±è´¥: {str(e)}")
            return "æŠ±æ­‰ï¼ŒæŸ¥è¯¢çŸ¥è¯†åº“æ—¶å‘ç”Ÿé”™è¯¯ã€‚ğŸ˜…", []
    
    def search_documents(self, query: str, top_k: int = 10) -> list:
        """
        ğŸ” æœç´¢çŸ¥è¯†åº“æ–‡æ¡£
        1. ğŸ” æ ¹æ®å…ƒæ•°æ®æœç´¢æ–‡æ¡£
        2. ğŸ“‹ è¿”å›åŒ¹é…çš„æ–‡æ¡£ä¿¡æ¯
        
        å‚æ•°:
          query: æœç´¢æŸ¥è¯¢
          top_k: è¿”å›ç»“æœæ•°é‡
        è¿”å›:
          åŒ¹é…çš„æ–‡æ¡£å…ƒæ•°æ®åˆ—è¡¨
        """
        try:
            logger.info(f"ğŸ” å¼€å§‹æ–‡æ¡£æœç´¢: '{query}'")
            
            # ğŸ” åœ¨å…ƒæ•°æ®ä¸­æœç´¢
            results = self.main_collection.get(
                where={"$or": [
                    {"source": {"$contains": query}},
                    {"author": {"$contains": query}},
                    {"tags": {"$contains": query}}
                ]},
                limit=top_k,
                include=["metadatas"]
            )
            
            # ğŸ“‹ æå–å”¯ä¸€æ–‡æ¡£ä¿¡æ¯
            unique_docs = {}
            for metadata in results['metadatas']:
                file_path = metadata.get('file_path')
                if file_path and file_path not in unique_docs:
                    unique_docs[file_path] = {
                        "source": metadata.get("source", ""),
                        "author": metadata.get("author", ""),
                        "tags": metadata.get("tags", ""),
                        "file_path": file_path
                    }
            
            doc_list = list(unique_docs.values())
            logger.info(f"âœ… æ–‡æ¡£æœç´¢å®Œæˆ: æ‰¾åˆ° {len(doc_list)} ä¸ªåŒ¹é…æ–‡æ¡£")
            return doc_list
        except Exception as e:
            logger.error(f"âŒ æ–‡æ¡£æœç´¢å¤±è´¥: {str(e)}")
            return []
    
    def get_document_chunks(self, file_path: str) -> list:
        """
        ğŸ“„ è·å–æ–‡æ¡£çš„æ‰€æœ‰å—
        1. ğŸ” æ ¹æ®æ–‡ä»¶è·¯å¾„æŸ¥æ‰¾æ‰€æœ‰ç›¸å…³å—
        2. ğŸ“‹ è¿”å›å—å†…å®¹å’Œå…ƒæ•°æ®
        
        å‚æ•°:
          file_path: æ–‡ä»¶è·¯å¾„
        è¿”å›:
          å—å†…å®¹åˆ—è¡¨
        """
        try:
            logger.info(f"ğŸ“„ è·å–æ–‡æ¡£å—: {file_path}")
            
            results = self.main_collection.get(
                where={"file_path": file_path},
                include=["documents", "metadatas"]
            )
            
            chunks = []
            for doc, meta in zip(results['documents'], results['metadatas']):
                chunks.append({
                    "content": doc,
                    "metadata": meta
                })
            
            logger.info(f"âœ… è·å–æ–‡æ¡£å—å®Œæˆ: {len(chunks)} ä¸ªå—")
            return chunks
        except Exception as e:
            logger.error(f"âŒ è·å–æ–‡æ¡£å—å¤±è´¥: {file_path} - {str(e)}")
            return []
    
    def update_chunk(self, chunk_id: str, new_content: str) -> bool:
        """
        âœï¸ æ›´æ–°çŸ¥è¯†å—å†…å®¹
        1. ğŸ”„ æ›´æ–°å‘é‡æ•°æ®åº“ä¸­çš„å†…å®¹
        2. ğŸ”„ é‡æ–°åµŒå…¥å‘é‡è¡¨ç¤º
        
        å‚æ•°:
          chunk_id: çŸ¥è¯†å—ID
          new_content: æ–°å†…å®¹
        è¿”å›:
          æ˜¯å¦æ›´æ–°æˆåŠŸ
        """
        try:
            logger.info(f"âœï¸ å¼€å§‹æ›´æ–°çŸ¥è¯†å—: {chunk_id}")
            
            self.main_collection.update(
                ids=[chunk_id],
                documents=[new_content]
            )
            logger.info(f"âœ… çŸ¥è¯†å—æ›´æ–°æˆåŠŸ: {chunk_id}")
            return True
        except Exception as e:
            logger.error(f"âŒ æ›´æ–°çŸ¥è¯†å—å¤±è´¥: {chunk_id} - {str(e)}")
            return False
    
    def get_statistics(self) -> dict:
        """
        ğŸ“Š è·å–çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯
        è¿”å›:
          åŒ…å«ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
        """
        try:
            logger.info("ğŸ“Š å¼€å§‹è·å–çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯")
            
            count = self.main_collection.count()
            
            # ğŸ“‹ è·å–æ‰€æœ‰å…ƒæ•°æ®
            results = self.main_collection.get(include=["metadatas"])
            metadatas = results['metadatas']
            
            # ğŸ“Š ç»Ÿè®¡æ–‡æ¡£ç±»å‹
            doc_types = {}
            for meta in metadatas:
                file_path = meta.get('file_path', '')
                if file_path:
                    file_type = Path(file_path).suffix[1:] or 'unknown'
                    doc_types[file_type] = doc_types.get(file_type, 0) + 1
            
            # ğŸ‘¥ ç»Ÿè®¡ä½œè€…
            authors = {}
            for meta in metadatas:
                author = meta.get('author', 'unknown')
                authors[author] = authors.get(author, 0) + 1
            
            stats = {
                "total_chunks": count,
                "document_types": doc_types,
                "authors": authors
            }
            
            logger.info(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯è·å–å®Œæˆ: æ€»å—æ•°={count}, æ–‡æ¡£ç±»å‹={len(doc_types)}, ä½œè€…æ•°={len(authors)}")
            return stats
        except Exception as e:
            logger.error(f"âŒ è·å–çŸ¥è¯†åº“ç»Ÿè®¡å¤±è´¥: {str(e)}")
            return {"error": "è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥"}


# ğŸŒ å…¨å±€çŸ¥è¯†åº“å®ä¾‹
knowledge_base = KnowledgeBase()